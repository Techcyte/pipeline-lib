import time
from typing import Tuple, Any, List, Iterable
import multiprocessing as mp
from queue import Empty
import traceback
import psutil

from .pipeline_task import PipelineTask
from .type_checking import type_check_tasks


def _process_dead(upstream_process_id: int):
    try:
        return psutil.Process(upstream_process_id).status() in (psutil.STATUS_DEAD, psutil.STATUS_ZOMBIE, psutil.STATUS_STOPPED)
    except Exception:
        return True


def _consume_queue(queue: mp.Queue, upstream_proc_ids: List[int])->Iterable[Any]:
    while True:
        try:
            item = queue.get(block=True, timeout=0.05)
            yield item
        except Empty:
            if all(_process_dead(id) for id in upstream_proc_ids):
                # parents are all dead, never going to get any more data, 
                # yield remaining data and exit quietly
                # main process will handle any errors 
                try:
                    while True:
                        yield queue.get_nowait()
                except Empty:
                    pass

                return


def _start_worker(task: PipelineTask, upstream_queue: mp.Queue, out_queue: mp.Queue, err_queue: mp.Queue, upstream_proc_ids: List[int]):
    # send items generated by tasks through pipe to parent
    try:
        constants = {} if task.constants is None else task.constants
        if upstream_queue is not None:
            generator_input = _consume_queue(upstream_queue, upstream_proc_ids)
            out_iter = task.generator(generator_input, **constants)
        else:
            out_iter = task.generator(**constants)

        if out_iter is not None:
            for item in out_iter:
                # if consumer dies, main will kill this process later, no need to check
                out_queue.put(item)
        
    except BaseException as e:
        tb = traceback.format_exc()
        err_queue.put((task.name, e, tb))
        raise e


class BadTaskExit(RuntimeError):
    """
    Indicates that a task had a non-zero exit code,
    despite not putting a message on the error queue
    """


def _cleanup_children(processes: List[mp.Process], err_queue: mp.Queue):
    # under normal execution, all processes will already be finished, so this would be a no-op
    # This is important if a task in the middle of the pipeline errors, so producers need to be killed
    # First, politely asks child processes to exit, then more forcefully kills them
    for proc in processes:
        proc.terminate()
    for proc in processes:
        proc.join(timeout=5.0)
    for proc in processes:
        proc.kill()
        # cleans up the task from the process table
        proc.join()
    
    # errors will all be on the queue at this time, get them without waiting
    errors = []
    while True:
        try:
            errors.append(err_queue.get_nowait())
        except Empty:
            break
    
    if errors:
        # reverse errors to print first/original errors at bottom of log
        for task_name, err, tb in reversed(errors):
            print(f"Error encountered in task '{task_name}'")
            print(tb)
            print(err)
        raise err
    
    if any(proc.exitcode != 0 for proc in processes):
        messages = []
        for proc in processes:
            if proc.exitcode != 0:
                messages.append(
                    f"Process {proc.name} exited with code {proc.exitcode} but did not raise an error, "
                    "likely was caused by a segfault in a c library.\n"
                )
        raise BadTaskExit("".join(messages))


def _start_tasks(tasks: List[PipelineTask]) -> Tuple[List[mp.Process], List[int], mp.Queue, mp.Queue]:
    """
    Starts up task
    """
    upstream_queue = None
    err_queue = mp.Queue()
    upstream_proc_ids = None

    processes: List[mp.Process] = []
    for task in tasks:
        downstream_queue = mp.Queue(maxsize=task.out_buffer_size)
        cur_proc_ids = []
        for proc_idx in range(task.num_procs):
            proc_name = f"{task.name}_{proc_idx}"
            taskproc = mp.Process(target=_start_worker, name=proc_name, args=(task, upstream_queue, downstream_queue, err_queue, upstream_proc_ids))
            taskproc.start()
            processes.append(taskproc)
            cur_proc_ids.append(taskproc.ident)

        upstream_queue = downstream_queue 
        upstream_proc_ids = cur_proc_ids

    final_proc_ids = cur_proc_ids
    return processes, final_proc_ids, err_queue


def execute(tasks: List[PipelineTask]):
    """
    execute tasks until final task completes. 
    Raises error if tasks are inconsistently specified or if 
    one of the tasks raises an error. 
    """
    if not tasks:
        return

    type_check_tasks(tasks)

    processes, final_pids, err_queue = _start_tasks(tasks)

    # wait for all consumer processes to finish
    while True:
        if all(_process_dead(id) for id in final_pids):
            break
        
        time.sleep(0.05)

    _cleanup_children(processes, err_queue)
