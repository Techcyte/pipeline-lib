"""

"""

from dataclasses import dataclass
import time
from typing import Callable, Type, Optional, Dict, Tuple, Any, List, Iterable
import inspect
import typing
import multiprocessing as mp
from queue import Empty
import traceback
import itertools
import warnings


@dataclass
class PipelineTask:
    generator: Callable
    constants: Optional[Dict[str, Any]]=None
    num_procs: int = 1
    out_buffer_size: int = 4

    @property
    def name(self):
        return self.generator.__name__


class PipelineTypeError(RuntimeError):
    pass


def type_error_if(condition, message):
    if not condition:
        raise PipelineTypeError(message)


def is_iterable(type: Type):
    return typing.get_origin(type) is typing.get_origin(Iterable)


def get_func_args(func):
    arguments = inspect.getfullargspec(func)
    type_error_if(arguments.varargs is None, "varargs not supported")
    type_error_if(arguments.varkw is None, "varkw not supported")
    type_error_if(arguments.defaults is None, "default arguments not supported")
    type_error_if(arguments.kwonlydefaults is None, "default arguments not supported")
    type_error_if(set(arguments.args + arguments.kwonlyargs).issubset(arguments.annotations), "all arguments must have annotations")
    type_error_if('return' in arguments.annotations, "function return type must have type annotation")
    
    base_input_type = None if not arguments.args else arguments.annotations[arguments.args[0]]
    base_return_type = arguments.annotations['return']

    type_error_if(base_input_type is None or (is_iterable(base_input_type) and len(typing.get_args(base_input_type)) == 1), "First argument must be an Iterable[input_type], if defined")
    type_error_if(base_return_type is None or (is_iterable(base_return_type) and len(typing.get_args(base_return_type)) == 1), "Return type annotation must be an Iterable[input_type] or None")

    input_type = None if base_input_type is None else typing.get_args(base_input_type)[0]
    return_type = None if base_return_type is None else typing.get_args(base_return_type)[0]

    # these are guarentteed to be mutually exclusive
    other_argument_names = arguments.args + arguments.kwonlyargs
    # remove input argument
    if arguments.args:
        other_argument_names.remove(arguments.args[0])

    return input_type, return_type, other_argument_names


def type_check_tasks(tasks: List[PipelineTask], last_is_none: bool = True):
    prev_type = None
    for task_idx, task in enumerate(tasks):
        input_type, return_type, other_args = get_func_args(task.generator)
        if prev_type != input_type:
            raise PipelineTypeError(f"In task {task.name}, expected input {input_type}, received input {prev_type}.")   

        if task_idx != len(tasks) - 1 and return_type is None:
            raise PipelineTypeError(f"None return type only allowed in final task of pipe")

        task_consts = {} if task.constants is None else task.constants
        task_const_names = list(task_consts.keys())
        if set(task_consts) != set(other_args):
            raise PipelineTypeError(f"In task {task.name}, expected constants {other_args}, received constants {task_const_names}.")    

        sanity_check_mp_params(task)

        prev_type = return_type

    if last_is_none and prev_type is not None:
        raise PipelineTypeError(f"In final task {task.name}, expected output type None, actual type {prev_type}.")   

    if not last_is_none and prev_type is None:
        raise PipelineTypeError(f"In final task {task.name}, expected iterator output, actual type {prev_type}.")   


def sanity_check_mp_params(task: PipelineTask):
    if task.num_procs <= 0:
        raise PipelineTypeError(f"In task {task.name}, num_procs value {task.num_procs} needs to be positive")
    if task.num_procs > mp.cpu_count():
        warnings.warn(f"In task {task.name}, num_procs value {task.num_procs} was greater than number of cpus on machine {mp.cpu_count()}")
    if task.out_buffer_size <= 0:
        raise PipelineTypeError(f"In task {task.name}, out_buffer_size {task.num_procs} needs to be positive")


class UpstreamError(RuntimeError):
    """
    One of the upstream processes stopped unexpectedly.
    """


def execute_child(tasks, pipe, err_pipe):
    # send items generated by tasks through pipe to parent
    try:
        for item in _execute_helper(tasks):
            pipe.send(item)
    except BaseException as e:
        tb = traceback.format_exc()
        err_pipe.send((e, tb))


def _execute_tasks_mp(tasks: List[PipelineTask], err_queue: mp.Queue) -> Optional[Iterable[Any]]:
    queue = mp.Queue()
    childproc = mp.Process(target=execute_child, args=(tasks, queue, err_queue))
    childproc.start()
    while True:
        yield queue.recv()


def _execute_helper(tasks: List[PipelineTask]) -> Optional[Iterable[Any]]:
    if not tasks:
        return
    my_task = tasks[-1]
    other_args = my_task.constants if my_task.constants else {}
    if len(tasks) == 1:
        return my_task.generator(**other_args)
    else:
        child_tasks = tasks[:-1]
        if child_tasks[-1].multiprocessing:
            iterator = _execute_tasks_mp(child_tasks)
        else:
            iterator = _execute_helper(child_tasks)
        return my_task.generator(iterator, **other_args)


def none_or_iterator(iter):
    """
    Utility to allow `execute` to either return None, or an iterator,
    depending on what  the last tasks is
    """
    try:
        # even if it is None, we still, unforutnately, need to call next() to start the generator
        first_item = next(iter)
        return itertools.chain([first_item], iter)
    except TypeError:
        # note that the error pipe will still be checked
        return None


def err_check_iter(iterator: Iterable[Any], err_queue: mp.Queue):
    try:
        for item in iterator:
            yield item
    except UpstreamError as current_err:
        # check for a root error in another process
        try:
            # if it errored in another process, two possiblities:
            # 1. Error is put on the queue before the process exits, therefore before UpstreamError exception is raised
            # 2. Error will never be put on the queue
            (task_name, err, tb) = err_queue.get_nowait()
            print(f"Error in task {task_name}")
            print(tb)
            raise err
        except Empty:
            raise UpstreamError("Upstream process in the pipeline exited without putting error on the error queue") from current_err

import psutil


def process_dead(upstream_process_id: int):
    try:
        return psutil.Process(upstream_process_id).status() in (psutil.STATUS_DEAD, psutil.STATUS_ZOMBIE, psutil.STATUS_STOPPED)
    except Exception:
        return True


def consume_queue(queue: mp.Queue, upstream_proc_ids: List[int])->Iterable[Any]:
    while True:
        try:
            item = queue.get(block=True, timeout=0.05)
            yield item
        except Empty:
            if any(process_dead(id) for id in upstream_proc_ids):
                # upstream might have died, and is not going to send us completely dependable data, exit quietly
                # main process will handle any errors 
                return


def start_worker(task: PipelineTask, upstream_queue: mp.Queue, out_queue: mp.Queue, err_queue: mp.Queue, upstream_proc_ids: List[int]):
    # send items generated by tasks through pipe to parent
    try:
        constants = {} if task.constants is None else task.constants
        if upstream_queue is not None:
            generator_input = consume_queue(upstream_queue, upstream_proc_ids)
            out_iter = task.generator(generator_input, **constants)
        else:
            out_iter = task.generator(**constants)

        if out_iter is not None:
            for item in out_iter:
                # if consumer dies, main will kill this process later, no need to check
                out_queue.put(item)
        
    except BaseException as e:
        tb = traceback.format_exc()
        err_queue.put((task.name, e, tb))
        raise e


def cleanup_children(processes: List[mp.Process], err_queue: mp.Queue):
    # under normal execution, all processes will already be finished, so this would be a no-op
    # This is important if a task in the middle of the pipeline errors, so producers need to be killed
    # First, politely asks child processes to exit, then more forcefully kills them
    for proc in processes:
        proc.terminate()
    for proc in processes:
        proc.join(timeout=5.0)
    for proc in processes:
        proc.kill()
        # cleans up the task from the process table
        proc.join()
    
    # errors will all be on the queue at this time, get them without waiting
    errors = []
    while True:
        try:
            errors.append(err_queue.get_nowait())
        except Empty:
            break
    
    if errors:
        # reverse errors to print first/original errors at bottom of log
        for task_name, err, tb in reversed(errors):
            print(f"Error encountered in task '{task_name}'")
            print(tb)
            print(err)
        raise err


def start_tasks(tasks: List[PipelineTask]) -> Tuple[List[mp.Process], List[int], mp.Queue, mp.Queue]:
    upstream_queue = None
    err_queue = mp.Queue()
    upstream_proc_ids = None

    processes: List[mp.Process] = []
    for task in tasks:
        downstream_queue = mp.Queue(maxsize=task.out_buffer_size)
        cur_proc_ids = []
        for _ in range(task.num_procs):
            taskproc = mp.Process(target=start_worker, args=(task, upstream_queue, downstream_queue, err_queue, upstream_proc_ids))
            taskproc.start()
            processes.append(taskproc)
            cur_proc_ids.append(taskproc.ident)

        upstream_queue = downstream_queue 
        upstream_proc_ids = cur_proc_ids
    
    final_proc_ids = cur_proc_ids
    return processes, final_proc_ids, upstream_queue, err_queue


def execute(tasks: List[PipelineTask]):
    """
    execute tasks until final task completes. 
    Raises error if tasks are inconsistently specified or if 
    one of the tasks raises an error. 
    """
    if not tasks:
        return

    type_check_tasks(tasks, last_is_none=True)

    processes, final_pids, upstream_queue, err_queue = start_tasks(tasks)

    # wait for all consumer processes to finish
    while True:
        if all(process_dead(id) for id in final_pids):
            break
        
        time.sleep(0.05)

    cleanup_children(processes, err_queue)


def yield_results(tasks: List[PipelineTask]) -> Iterable[Any]:
    """
    execute tasks and yields results to user 
    Raises error if tasks are inconsistently specified or if 
    one of the tasks raises an error. 
    """

    if not tasks:
        return

    type_check_tasks(tasks, last_is_none=False)

    processes, final_pids, upstream_queue, err_queue = start_tasks(tasks)

    yield from consume_queue(upstream_queue, final_pids)

    cleanup_children(processes, err_queue)
