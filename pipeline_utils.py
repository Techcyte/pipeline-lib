"""

"""

from dataclasses import dataclass
import time
from typing import Callable, Type, Optional, Dict, Any, List, Iterable
import inspect
import typing
import multiprocessing as mp
from queue import Empty
import traceback
import itertools


@dataclass
class PipelineTask:
    generator: Callable
    constants: Optional[Dict[str, Any]]=None
    num_procs: int = 0

    @property
    def name(self):
        return self.generator.__name__


class PipelineTypeError(RuntimeError):
    pass


def type_error_if(condition, message):
    if not condition:
        raise PipelineTypeError(message)


def is_iterable(type: Type):
    return typing.get_origin(type) is typing.get_origin(Iterable)


def get_func_args(func):
    arguments = inspect.getfullargspec(func)
    type_error_if(arguments.varargs is None, "varargs not supported")
    type_error_if(arguments.varkw is None, "varkw not supported")
    type_error_if(arguments.defaults is None, "default arguments not supported")
    type_error_if(arguments.kwonlydefaults is None, "default arguments not supported")
    type_error_if(set(arguments.args + arguments.kwonlyargs).issubset(arguments.annotations), "all arguments must have annotations")
    type_error_if('return' in arguments.annotations, "function return type must have type annotation")
    
    base_input_type = None if not arguments.args else arguments.annotations[arguments.args[0]]
    base_return_type = arguments.annotations['return']

    type_error_if(base_input_type is None or (is_iterable(base_input_type) and len(typing.get_args(base_input_type)) == 1), "First argument must be an Iterable[input_type], if defined")
    type_error_if(base_return_type is None or (is_iterable(base_return_type) and len(typing.get_args(base_return_type)) == 1), "Return type annotation must be an Iterable[input_type] or None")

    input_type = None if base_input_type is None else typing.get_args(base_input_type)[0]
    return_type = None if base_return_type is None else typing.get_args(base_return_type)[0]

    # these are guarentteed to be mutually exclusive
    other_argument_names = arguments.args + arguments.kwonlyargs
    # remove input argument
    if arguments.args:
        other_argument_names.remove(arguments.args[0])

    return input_type, return_type, other_argument_names


def type_check_tasks(tasks: List[PipelineTask]):
    prev_type = None
    for task_idx, task in enumerate(tasks):
        input_type, return_type, other_args = get_func_args(task.generator)
        if prev_type != input_type:
            raise PipelineTypeError(f"In task {task.name}, expected input {input_type}, received input {prev_type}.")   

        if task_idx != len(tasks) - 1 and return_type is None:
            raise PipelineTypeError(f"None return type only allowed in final task of pipe")

        task_consts = {} if task.constants is None else task.constants
        task_const_names = list(task_consts.keys())
        if set(task_consts) != set(other_args):
            raise PipelineTypeError(f"In task {task.name}, expected constants {other_args}, received constants {task_const_names}.")    

        prev_type = return_type

    # if prev_type != None:
    #     raise PipelineTypeError(f"In final task {task.name}, expected output type None, actual type {prev_type}.")   


class UpstreamError(RuntimeError):
    """
    One of the upstream processes stopped unexpectedly.
    """


def execute_child(tasks, pipe, err_pipe):
    # send items generated by tasks through pipe to parent
    try:
        for item in _execute_helper(tasks):
            pipe.send(item)
    except BaseException as e:
        tb = traceback.format_exc()
        err_pipe.send((e, tb))


def _execute_tasks_mp(tasks: List[PipelineTask], err_queue: mp.Queue) -> Optional[Iterable[Any]]:
    queue = mp.Queue()
    childproc = mp.Process(target=execute_child, args=(tasks, queue, err_queue))
    childproc.start()
    while True:
        yield queue.recv()


def _execute_helper(tasks: List[PipelineTask]) -> Optional[Iterable[Any]]:
    if not tasks:
        return
    my_task = tasks[-1]
    other_args = my_task.constants if my_task.constants else {}
    if len(tasks) == 1:
        return my_task.generator(**other_args)
    else:
        child_tasks = tasks[:-1]
        if child_tasks[-1].multiprocessing:
            iterator = _execute_tasks_mp(child_tasks)
        else:
            iterator = _execute_helper(child_tasks)
        return my_task.generator(iterator, **other_args)


def none_or_iterator(iter):
    """
    Utility to allow `execute` to either return None, or an iterator,
    depending on what  the last tasks is
    """
    try:
        # even if it is None, we still, unforutnately, need to call next() to start the generator
        first_item = next(iter)
        return itertools.chain([first_item], iter)
    except TypeError:
        # note that the error pipe will still be checked
        return None


def err_check_iter(iterator: Iterable[Any], err_queue: mp.Queue):
    try:
        for item in iterator:
            yield item
    except UpstreamError as current_err:
        # check for a root error in another process
        try:
            # if it errored in another process, two possiblities:
            # 1. Error is put on the queue before the process exits, therefore before UpstreamError exception is raised
            # 2. Error will never be put on the queue
            (task_name, err, tb) = err_queue.get_nowait()
            print(f"Error in task {task_name}")
            print(tb)
            raise err
        except Empty:
            raise UpstreamError("Upstream process in the pipeline exited without putting error on the error queue") from current_err

import psutil


def process_dead(upstream_process_id: int):
    try:
        return psutil.Process(upstream_process_id).status() in (psutil.STATUS_DEAD, psutil.STATUS_ZOMBIE, psutil.STATUS_STOPPED)
    except Exception:
        return True


def consume_queue(queue: mp.Queue, upstream_proc_id: int)->Iterable[Any]:
    while True:
        try:
            item = queue.get(block=True, timeout=0.1)
            yield item
        except Empty:
            if process_dead(upstream_proc_id):
                # upstream is not going to send us any data, exit quietly
                # main process will handle any errors 
                return


def start_worker(task: PipelineTask, upstream_queue: mp.Queue, out_queue: mp.Queue, err_queue: mp.Queue, upstream_proc: int):
    # send items generated by tasks through pipe to parent
    try:
        constants = {} if task.constants is None else task.constants
        if upstream_queue is not None:
            generator_input = consume_queue(upstream_queue, upstream_proc)
            out_iter = task.generator(generator_input, **constants)
        else:
            out_iter = task.generator(**constants)

        if out_iter is not None:
            for item in out_iter:
                # if consumer dies, main will kill this process later, no need to check
                out_queue.put(item)
        
    except BaseException as e:
        tb = traceback.format_exc()
        err_queue.put((task.name, e, tb))
        raise e


def cleanup_children(processes: List[mp.Process], err_queue: mp.Queue):
    # under normal execution, all processes will already be finished, so this would be a no-op
    # This is important if a task in the middle of the pipeline errors, so producers need to be killed
    # First, politely asks child processes to exit, then more forcefully kills them
    for proc in processes:
        proc.terminate()
    for proc in processes:
        proc.join(timeout=5.0)
    for proc in processes:
        proc.kill()
        # cleans up the task from the process table
        proc.join()
    
    # errors will all be on the queue at this time, get them without waiting
    errors = []
    while True:
        try:
            errors.append(err_queue.get_nowait())
        except Empty:
            break
    
    if errors:
        # reverse errors to print first/original errors at bottom of log
        for task_name, err, tb in reversed(errors):
            print(f"Error encountered in task '{task_name}'")
            print(tb)
            print(err)
        raise err


def finalize_queue(processes: List[mp.Process], final_queue: mp.Queue, final_pid: int, err_queue: mp.Queue):
    yield from consume_queue(final_queue, final_pid)
    cleanup_children(processes, err_queue)

def execute(tasks: List[PipelineTask]):
    """
    execute tasks until final task completes. Garbage collector will 
    clean up remainder of generators by raising a error
    """
    if not tasks:
        return

    type_check_tasks(tasks)

    processes: List[mp.Process] = []

    upstream_queue = None
    err_queue = mp.Queue()
    downstream_queue = mp.Queue()
    upstream_proc_id = None
    for task in tasks:
        taskproc = mp.Process(target=start_worker, args=(task, upstream_queue, downstream_queue, err_queue, upstream_proc_id))
        taskproc.start()
        processes.append(taskproc)

        upstream_proc_id = taskproc.ident
        upstream_queue = downstream_queue 
        downstream_queue = mp.Queue()
    
    my_task = tasks[-1].generator
    _, return_type, _ = get_func_args(my_task)

    last_child = taskproc
    if return_type is not None: 
        return finalize_queue(processes, upstream_queue, last_child.ident, err_queue)
    else:
        while True:
            if not last_child.is_alive():
                break
            
            time.sleep(0.05)

        cleanup_children(processes, err_queue)
    