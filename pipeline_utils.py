"""

"""

from dataclasses import dataclass
from typing import Callable, Type, Optional, Dict, Any, List, Iterable, Union
from multiprocessing import Queue
import inspect
import typing
import pytest
import multiprocessing as mp
import os


@dataclass
class PipelineTask:
    generator: Callable
    constants: Optional[Dict[str, Any]]=None
    multiprocessing: bool = False

    @property
    def name(self):
        return self.generator.__name__




class PipelineTypeError(RuntimeError):
    pass


def type_error_if(condition, message):
    if not condition:
        raise PipelineTypeError(message)


def is_iterable(type: Type):
    return typing.get_origin(Iterable[str]) is typing.get_origin(Iterable)


def get_func_args(func):
    arguments = inspect.getfullargspec(func)
    type_error_if(arguments.varargs is None, "varargs not supported")
    type_error_if(arguments.varkw is None, "varkw not supported")
    type_error_if(arguments.defaults is None, "default arguments not supported")
    type_error_if(arguments.kwonlydefaults is None, "default arguments not supported")
    type_error_if(set(arguments.args + arguments.kwonlyargs).issubset(arguments.annotations), "all arguments must have annotations")
    type_error_if('return' in arguments.annotations, "function return type must have type annotation")
    
    base_input_type = None if not arguments.args else arguments.annotations[arguments.args[0]]
    base_return_type = arguments.annotations['return']

    type_error_if(base_input_type is None or (is_iterable(base_input_type) and len(typing.get_args(base_input_type)) == 1), "First argument must be an Iterable[input_type], if defined")
    type_error_if(base_return_type is None or (is_iterable(base_return_type) and len(typing.get_args(base_return_type)) == 1), "Return type annotation must be an Iterable[input_type] or None")

    input_type = None if base_input_type is None else typing.get_args(base_input_type)[0]
    return_type = None if base_return_type is None else typing.get_args(base_return_type)[0]

    # these are guarentteed to be mutually exclusive
    other_argument_names = arguments.args + arguments.kwonlyargs
    # remove input argument
    if arguments.args:
        other_argument_names.remove(arguments.args[0])

    return input_type, return_type, other_argument_names




def type_check_tasks(tasks: List[PipelineTask]):
    prev_type = None
    for task in tasks:
        input_type, return_type, other_args = get_func_args(task.generator)
        if prev_type != input_type:
            raise PipelineTypeError(f"In task {task.name}, expected input {input_type}, received input {prev_type}.")   

        task_consts = {} if task.constants is None else task.constants
        task_const_names = list(task_consts.keys())
        if set(task_consts) != set(other_args):
            raise PipelineTypeError(f"In task {task.name}, expected constants {other_args}, received constants {task_const_names}.")    

        prev_type = return_type

    # final result can be an iterator or None, already checked by get_func_args
    



def iter_connection(conn):
    try:
        while True:
            yield conn.recv()
    except EOFError:
        # EOFError is expected on normal end of iteration
        return

def execute_child(pipe, tasks):
    # send items generated by tasks through pipe to parent
    for item in _execute_helper(tasks):
        pipe.send(item)


def _execute_helper(tasks: List[PipelineTask]) -> Optional[Iterable[Any]]:
    if not tasks:
        return
    my_task = tasks[-1]
    other_args = my_task.constants if my_task.constants else {}
    if len(tasks) == 1:
        return my_task.generator(**other_args)
    else:
        child_tasks = tasks[:-1]
        if my_task.multiprocessing:
            child_pipe, parent_pipe = mp.Pipe()
            childproc = mp.Process(target=execute_child, args=(child_pipe, child_tasks))
            childproc.start()
            # need to close parent's end of child pipe so that the pipe closes when the child exits
            child_pipe.close()
            iterator = iter_connection(parent_pipe)
        else:
            iterator = _execute_helper(child_tasks)
        return my_task.generator(iterator, **other_args)        


def execute(tasks: List[PipelineTask]) -> Optional[Iterable[Any]]:
    """
    execute tasks until final task completes. Garbage collector will 
    clean up remainder of generators by raising a error
    """
    if not tasks:
        return

    type_check_tasks(tasks)
    # type checking done at this point, now don't assume types all there
    return _execute_helper(tasks)
