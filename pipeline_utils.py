"""

"""

from dataclasses import dataclass
from typing import Callable, Type, Optional, Dict, Any, List, Iterable
import inspect
import typing
import multiprocessing as mp
import traceback
import itertools


@dataclass
class PipelineTask:
    generator: Callable
    constants: Optional[Dict[str, Any]]=None
    multiprocessing: bool = False

    @property
    def name(self):
        return self.generator.__name__


class PipelineTypeError(RuntimeError):
    pass


def type_error_if(condition, message):
    if not condition:
        raise PipelineTypeError(message)


def is_iterable(type: Type):
    return typing.get_origin(type) is typing.get_origin(Iterable)


def get_func_args(func):
    arguments = inspect.getfullargspec(func)
    type_error_if(arguments.varargs is None, "varargs not supported")
    type_error_if(arguments.varkw is None, "varkw not supported")
    type_error_if(arguments.defaults is None, "default arguments not supported")
    type_error_if(arguments.kwonlydefaults is None, "default arguments not supported")
    type_error_if(set(arguments.args + arguments.kwonlyargs).issubset(arguments.annotations), "all arguments must have annotations")
    type_error_if('return' in arguments.annotations, "function return type must have type annotation")
    
    base_input_type = None if not arguments.args else arguments.annotations[arguments.args[0]]
    base_return_type = arguments.annotations['return']

    type_error_if(base_input_type is None or (is_iterable(base_input_type) and len(typing.get_args(base_input_type)) == 1), "First argument must be an Iterable[input_type], if defined")
    type_error_if(base_return_type is None or (is_iterable(base_return_type) and len(typing.get_args(base_return_type)) == 1), "Return type annotation must be an Iterable[input_type] or None")

    input_type = None if base_input_type is None else typing.get_args(base_input_type)[0]
    return_type = None if base_return_type is None else typing.get_args(base_return_type)[0]

    # these are guarentteed to be mutually exclusive
    other_argument_names = arguments.args + arguments.kwonlyargs
    # remove input argument
    if arguments.args:
        other_argument_names.remove(arguments.args[0])

    return input_type, return_type, other_argument_names


def type_check_tasks(tasks: List[PipelineTask]):
    prev_type = None
    for task_idx, task in enumerate(tasks):
        input_type, return_type, other_args = get_func_args(task.generator)
        if prev_type != input_type:
            raise PipelineTypeError(f"In task {task.name}, expected input {input_type}, received input {prev_type}.")   

        if task_idx != len(tasks) - 1 and return_type is None:
            raise PipelineTypeError(f"None return type only allowed in final task of pipe")

        task_consts = {} if task.constants is None else task.constants
        task_const_names = list(task_consts.keys())
        if set(task_consts) != set(other_args):
            raise PipelineTypeError(f"In task {task.name}, expected constants {other_args}, received constants {task_const_names}.")    

        prev_type = return_type

    # if prev_type != None:
    #     raise PipelineTypeError(f"In final task {task.name}, expected output type None, actual type {prev_type}.")   
    


def execute_child(tasks, pipe, err_pipe):
    # send items generated by tasks through pipe to parent
    try:
        for item in _execute_helper(tasks):
            pipe.send(item)
    except BaseException as e:
        tb = traceback.format_exc()
        err_pipe.send((e, tb))


def _execute_tasks_mp(tasks: List[PipelineTask]) -> Optional[Iterable[Any]]:
    child_pipe, parent_pipe = mp.Pipe()
    child_err_pipe, parent_err_pipe = mp.Pipe()
    childproc = mp.Process(target=execute_child, args=(tasks, child_pipe, child_err_pipe))
    childproc.start()
    # need to close parent's version of child pipe so that the parent doesn't own the file descriptor,
    # allowing the OS to automatically close the pipe when the child exits, causing an error in the parent if it is
    # waiting on the child
    child_pipe.close()
    child_err_pipe.close()
    try:
        while True:
            yield parent_pipe.recv()
    except EOFError:
        # EOF error is expected on normal end of execution, but it could also indicate an error
        has_error = False
        try:
            # note that this logic only works if the message fits inside the 65k buffer
            (err, tb) = parent_err_pipe.recv()
            # set here and raise outside try block to differentiate EOF here from EOF in child
            has_error = True
        except EOFError:
            # an EOF on the error pipe means no error was logged, indicating normal end of execution 
            pass

        if has_error:
            print("traceback received from child, reraising:")
            print(tb)
            raise err
    finally:
        # in case child is still running, close parent pipe. 
        # This should cause child to error and to exit
        # when next attempts to write to this pipe
        parent_pipe.close()
        parent_err_pipe.close()
        # waits for child to exit and cleans up the process completely.
        # We would rather wait for awhile than to have unlimited daemon processes running around the system 
        childproc.join()

def _execute_helper(tasks: List[PipelineTask]) -> Optional[Iterable[Any]]:
    if not tasks:
        return
    my_task = tasks[-1]
    other_args = my_task.constants if my_task.constants else {}
    if len(tasks) == 1:
        return my_task.generator(**other_args)
    else:
        child_tasks = tasks[:-1]
        if child_tasks[-1].multiprocessing:
            iterator = _execute_tasks_mp(child_tasks)
        else:
            iterator = _execute_helper(child_tasks)
        return my_task.generator(iterator, **other_args)


def none_or_iterator(iter):
    """
    Utility to allow `execute` to either return None, or an iterator,
    depending on what  the last tasks is
    """
    try:
        # even if it is None, we still, unforutnately, need to call next() to start the generator
        first_item = next(iter)
        return itertools.chain([first_item], iter)
    except TypeError:
        # note that the error pipe will still be checked
        return None


def execute(tasks: List[PipelineTask]):
    """
    execute tasks until final task completes. Garbage collector will 
    clean up remainder of generators by raising a error
    """
    if not tasks:
        return

    type_check_tasks(tasks)

    if tasks[-1].multiprocessing:
        return none_or_iterator(_execute_tasks_mp(tasks))
    else:
        return _execute_helper(tasks)
    